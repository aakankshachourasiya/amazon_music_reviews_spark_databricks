{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e7d030f-e842-46a4-80b4-b61b54534d80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Packages\n",
    "Run this cell before doing anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c7f513f-64ab-4667-afa2-e72aa073af9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#import necessary packages\n",
    "\n",
    "import pyspark as ps\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import to_date, dayofmonth, month, year, regexp_replace, regexp_extract, col, trim, when, length, lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "621da29c-43b4-469c-9cae-040a97de0b91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Preprocessing\n",
    "No need to run this part again. To skip this part, Use df10 loaded in DBFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e88e3d00-ecb3-4754-865a-c6a654151fab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#creating Spark Dataframe from uploaded data\n",
    "df = spark.read.json(\"dbfs:/FileStore/tables/CDs_and_Vinyl_5.json\")\n",
    "\n",
    "#Show data to see what it looks like before cleaning\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5af7ddef-befc-4add-9e4d-07d97beabe37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Display data to see what it looks like and what data type is assigned\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43a1bd93-cb17-46c2-87b1-7a24010f9ae5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#count number of observations\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cbdc77d-6442-4ba9-a69c-9146edec6a5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#drops data rows with Style = NULL (no idea whether it is a cd,dvd,mp3 etc)\n",
    "df2 = df.dropna(subset=['style'])\n",
    "\n",
    "#check how many datarows\n",
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "689827c4-ffe4-4d19-ac74-8c13d9454ab4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#dropping irrelevant variables, reasons as follows:Image cannot be seen in DF, reviewerName is personal data and we can use reviewerID instead, unixReviewTime is a duplicate of reviewTime, which will be cleaned in next step.\n",
    "df3 = df2.drop('image', 'reviewerName', 'unixReviewTime')\n",
    "\n",
    "#Cleaning of the date/time and converting it to a date/time object\n",
    "#Replace double spaces (or more) with a single space \n",
    "#Generated with ChatGPT prompts:\n",
    "#  \"convert this data in pyspark to date/time type\" (with image of data)\n",
    "#  \"getting an error because one of the days has a single digit\"\n",
    "df4 = df3.withColumn(\"reviewTime_cleaned\", regexp_replace(\"reviewTime\", \"\\s+\", \" \"))\n",
    "\n",
    "#Convert to date using flexible format \n",
    "df4 = df4.withColumn(\"reviewTime\", to_date(\"reviewTime_cleaned\", \"M d, yyyy\"))\n",
    "\n",
    "#Drop the cleaned helper column\n",
    "df4 = df4.drop(\"reviewTime_cleaned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4911c5f-8674-4852-88d0-9525a13ba91a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#split date/time into day/month/year columns\n",
    "#generated with ChatGPT prompt:\n",
    "#\"convert date/time object into 3 separate columns day month year in pyspark\"\n",
    "\n",
    "df45 = df4.withColumn(\"reviewDay\", dayofmonth(\"reviewTime\")) \\\n",
    "       .withColumn(\"reviewMonth\", month(\"reviewTime\")) \\\n",
    "       .withColumn(\"reviewYear\", year(\"reviewTime\")) \\\n",
    "        .drop('reviewTime')\n",
    "\n",
    "#check data\n",
    "df45.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21599cd0-743b-478c-8dc9-52ddcfff4f58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Cleaning \"style\" column from object to string\n",
    "\n",
    "# Overwrite 'style' column with cleaned format\n",
    "df5 = df45.withColumn(\"style\", trim(col(\"style.`Format:`\")))\n",
    "display(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f06416c-e742-4cc2-b2a2-07d704c9abe7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#change column \"vote\" from string to double, and replace \"null\" with \"0\"\n",
    "#generated with following ChatGPT prompt:\n",
    "# \"Convert a \"string\" column to \"double\" in PySpark and switch \"null\" values to \"0\" in \"helpful\" column\"\n",
    "\n",
    "df6 = df5.withColumn(\n",
    "    \"vote\",\n",
    "    when(col(\"vote\").isNull(), 0.0)  # Replace null with 0.0\n",
    "    .otherwise(col(\"vote\").cast(\"double\"))  # Cast string to double\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73df3772-15f3-4f7f-af63-42d95a445996",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#renaming columns\n",
    "\n",
    "# List of tuples (old_column_name, new_column_name)\n",
    "columns_to_rename = [(\"asin\", \"amazon_productid\"), (\"overall\", \"rating\"), (\"vote\", \"helpful\"), (\"summary\", \"reviewSummary\")]\n",
    "\n",
    "# Renaming columns\n",
    "for old_name, new_name in columns_to_rename:\n",
    "    df6 = df6.withColumnRenamed(old_name, new_name)\n",
    "\n",
    "df6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a7bc2d3-cf7f-4dfa-a99d-5460c63de903",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#reordering columns\n",
    "\n",
    "new_order = [\"amazon_productid\", \"style\", \"rating\", \"reviewDay\", \"reviewMonth\", \"reviewYear\", \"reviewerID\", \"verified\", \"helpful\", \"reviewSummary\", \"reviewText\"]\n",
    "df7 = df6.select(*new_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eaecde94-80f2-4bcb-b770-ee75f169a27d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Removing duplicates and keeps only reviews with more than 20 character\n",
    "#adapted from ChatGPT prompt to suggest further ideas for data-cleaning\n",
    "df8 = df7.dropDuplicates([\"reviewText\", \"reviewerID\", \"amazon_productid\"])\n",
    "df9 = df8.filter((col(\"reviewText\").isNotNull()) & (length(col(\"reviewText\")) > 20))\n",
    "df9.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b389b6af-3b8d-4c8b-9f2d-0624d912a4d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#standardising text columns; lowercasing, removing punctuation, trimming whitespace. (generated)\n",
    "#adapted from ChatGPT prompt to suggest further ideas for data-cleaning\n",
    "df10 = df9.withColumn(\"reviewText\", lower(regexp_replace(col(\"reviewText\"), \"[^a-zA-Z0-9\\s]\", \"\")))\n",
    "df10 = df10.withColumn(\"reviewSummary\", lower(regexp_replace(col(\"reviewSummary\"), \"[^a-zA-Z0-9\\s]\", \"\")))\n",
    "display(df10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc27c228-9050-4a6d-8c1b-e41f8477c651",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Save to DBFS for easy reading later on\n",
    "#ChatGPT prompt: how to save df10, a spark dataframe, to a dbfs in databricks community edition\n",
    "\n",
    "df10.write.mode(\"overwrite\").parquet(\"/dbfs/df10_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98231589-cd8c-4962-ba95-c6aff0ee1ce3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Column descriptors:\n",
    "- Amazon Product ID = refers to a particular product on Amazon\n",
    "- Style = type of product i.e. CD, MP3, etc\n",
    "- Rating = from 1 to 5, 5 being highest and 1 being lowest\n",
    "- reviewDay/Month/Year - selfexplanatory\n",
    "- reviewerID = anonymised identifier of reviewer.\n",
    "- verified = whether review is verified by Amazon.\n",
    "- helpful = amount of 'helpful' votes received by review\n",
    "- reviewSummary = title of review\n",
    "- reviewText = full review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91a4f497-6d14-4dcf-a35a-ba7993c35603",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Exploratory Data Analysis - Insightful visualizations and statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2eb51f61-e672-4187-8b55-220ac6efa1f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, Tokenizer, StopWordsRemover, CountVectorizer, IDF\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.stat import Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4eec98a2-74f3-48e2-b793-cb04b8fd6d7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the saved df10 from DBFS\n",
    "df10 = spark.read.parquet(\"/dbfs/df10_output\")\n",
    "\n",
    "# 0. SAMPLE DATA FROM SPARK\n",
    "total_rows = df10.count()\n",
    "sample_fraction = 10000 / total_rows\n",
    "\n",
    "# Sample a subset of the data\n",
    "#generated with following ChatGPT prompt: How to make the subset of the large data for visualisation.\n",
    "df_sampled = df10.sample(withReplacement=False, fraction=sample_fraction, seed=42)\n",
    "\n",
    "# Cache the sampled data for subsequent transformations\n",
    "df_sampled.cache()\n",
    "\n",
    "# Display sample using Databricks built-in visualizer\n",
    "display(df_sampled)\n",
    "\n",
    "# Convert to Pandas for local analysis\n",
    "pdf = df_sampled.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "090c931f-6ba8-42b1-ada4-6c4fe596ac21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. DISTRIBUTION ANALYSIS \n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_palette(\"deep\")\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Step 1: Distribution of Ratings\n",
    "sns.histplot(pdf['rating'], kde=True, ax=axs[0, 0], bins=5, discrete=True)\n",
    "axs[0, 0].set_title('Distribution of Ratings')\n",
    "axs[0, 0].set_xlabel('Rating')\n",
    "axs[0, 0].set_ylabel('Count')\n",
    "axs[0, 0].set_xticks(range(1, 6))\n",
    "\n",
    "# Step 2: Pie chart: Verified vs Non-Verified\n",
    "verified_counts = pdf['verified'].value_counts()\n",
    "axs[0, 1].pie(verified_counts, labels=['Not Verified', 'Verified'], autopct='%1.1f%%', startangle=90, colors=['#66c2a5', '#fc8d62'])\n",
    "axs[0, 1].set_title('Verified vs Non-Verified Reviews')\n",
    "\n",
    "# Step 3: Top 10 Product Styles\n",
    "style_counts = pdf['style'].value_counts().head(10)\n",
    "style_counts.sort_values().plot(kind='barh', ax=axs[1, 0])\n",
    "axs[1, 0].set_title('Top 10 Product Styles')\n",
    "axs[1, 0].set_xlabel('Count')\n",
    "axs[1, 0].set_ylabel('Style')\n",
    "\n",
    "# Step 4: Average Helpful Votes by Rating\n",
    "avg_helpful_by_rating = pdf.groupby('rating')['helpful'].mean()\n",
    "\n",
    "# Plot the bar chart for Average Helpful Votes by Rating\n",
    "axs[1, 1].bar(avg_helpful_by_rating.index, avg_helpful_by_rating.values, color=sns.color_palette(\"deep\")[0])\n",
    "axs[1, 1].set_title('Average Helpful Votes by Rating')\n",
    "axs[1, 1].set_xlabel('Rating')\n",
    "axs[1, 1].set_ylabel('Avg Helpful Votes')\n",
    "\n",
    "# Set the x-ticks to be from 1 to 5 \n",
    "axs[1, 1].set_xticks(range(1, 6)) \n",
    "axs[1, 1].set_xticklabels(range(1, 6))  \n",
    "\n",
    "# Adjust layout to ensure all plots fit well\n",
    "plt.tight_layout()\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "876b552c-7b23-4ad8-b624-76a0c2561dfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. TEMPORAL ANALYSIS\n",
    "pdf['review_date'] = pd.to_datetime(dict(year=pdf['reviewYear'], \n",
    "                                         month=pdf['reviewMonth'], \n",
    "                                         day=pdf['reviewDay']))\n",
    "\n",
    "annual_reviews = pdf.groupby(pdf['review_date'].dt.year).size()\n",
    "\n",
    "annual_reviews.plot()\n",
    "plt.title('Annual Review Volume')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Reviews')\n",
    "\n",
    "# Remove decimal points from x-axis (prompt: remove decimal points from this graph)\n",
    "plt.gca().xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "892596ed-f00f-42a0-8ebd-803639b7a92f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. TEXT ANALYSIS\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Extend stopwords with domain-specific common words\n",
    "custom_stopwords = set(ENGLISH_STOP_WORDS).union({\n",
    "    'product', 'amazon', 'buy', 'purchase', 'purchased', 'use', 'used', 'using', \n",
    "    'thing', 'really', 'get', 'got', 'would', 'one', 'also', 'could', 'even',\n",
    "    'item', 'well', 'still', 'time', 'album', 'music', 'songs','im'\n",
    "})\n",
    "\n",
    "# Clean text function\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Plot top words function\n",
    "#generated with following ChatGPT prompt: How to plot most frequent words in the review for positive and negative\n",
    "def plot_top_words(text_series, title, color, top_n=10, ax=None):\n",
    "    text = ' '.join(text_series.fillna('').astype(str).map(clean_text))\n",
    "    words = [word for word in text.split() if word not in custom_stopwords]\n",
    "    word_counts = pd.Series(words).value_counts().head(top_n)\n",
    "    \n",
    "    sns.barplot(x=word_counts.values, y=word_counts.index, palette=color, ax=ax)\n",
    "    ax.set_title(title, fontsize=16)\n",
    "    ax.set_xlabel('Word Frequency')\n",
    "    ax.set_ylabel('Word')\n",
    "\n",
    "# Assuming 'pdf' is your DataFrame containing the reviews and ratings\n",
    "positive_reviews = pdf[pdf['rating'] == 5]['reviewText']\n",
    "negative_reviews = pdf[pdf['rating'] == 1]['reviewText']\n",
    "\n",
    "# Create subplots for both graphs in one row\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot top words for positive and negative reviews side by side\n",
    "plot_top_words(positive_reviews, 'Top 10 Words in Positive Reviews', 'Greens_r', ax=axes[0])\n",
    "plot_top_words(negative_reviews, 'Top 10 Words in Negative Reviews', 'Reds_r', ax=axes[1])\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dde53a4a-4a1c-412b-9222-f64686e3b3d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. SUMMARY METRICS\n",
    "summary_stats = {\n",
    "    'Total Reviews': len(pdf),\n",
    "    'Average Rating': pdf['rating'].mean(),\n",
    "    'Verified Reviews (%)': (pdf['verified'] == True).mean() * 100,\n",
    "    'Average Helpful Votes': pdf['helpful'].mean(),\n",
    "    'Most Common Style': pdf['style'].value_counts().index[0],\n",
    "    'Most Active Year': pdf['reviewYear'].value_counts().index[0]\n",
    "}\n",
    "\n",
    "print(\"\\n===== DATASET SUMMARY =====\")\n",
    "for key, value in summary_stats.items():\n",
    "    print(f\"{key}: {value:.2f}\" if isinstance(value, float) else f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5efba5c-17df-45c2-b683-e011e99d8180",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5. SPARKSQL SUMMARY\n",
    "df10.createOrReplaceTempView(\"reviews\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        AVG(rating) AS avg_rating,\n",
    "        STDDEV(rating) AS stddev_rating,\n",
    "        MIN(helpful) AS min_helpful,\n",
    "        MAX(helpful) AS max_helpful,\n",
    "        AVG(helpful) AS avg_helpful,\n",
    "        PERCENTILE(helpful, 0.5) AS median_helpful\n",
    "    FROM reviews\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aabaa4ad-36f7-4123-a09a-b08e81b95b8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e60e2914-5ef5-4468-a9ed-658eaf7f918d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. CORRELATION ANALYSIS\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Read the saved df10 from DBFS\n",
    "df10 = spark.read.parquet(\"/dbfs/df10_output\")\n",
    "\n",
    "# Ensure 'rating' and 'helpful' are DoubleType and not null\n",
    "#generated with following ChatGPT prompt: How can I find correlation between rating & helpful by using optimisation \n",
    "df_filtered = df10 \\\n",
    "    .filter((col(\"rating\").isNotNull()) & (col(\"helpful\").isNotNull())) \\\n",
    "    .withColumn(\"rating\", col(\"rating\").cast(DoubleType())) \\\n",
    "    .withColumn(\"helpful\", col(\"helpful\").cast(DoubleType()))\n",
    "\n",
    "# Assemble features\n",
    "assembler = VectorAssembler(inputCols=['rating', 'helpful'], outputCol='features')\n",
    "assembled_df = assembler.transform(df_filtered).select(\"features\")\n",
    "\n",
    "# Cache to avoid recomputing\n",
    "assembled_df.cache()\n",
    "\n",
    "# Correlation matrix\n",
    "correlation_matrix = Correlation.corr(assembled_df, 'features', 'pearson')\n",
    "correlation_matrix.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f448e034-6f59-4afe-9c90-8f27bfb69a9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2. NORMALIZATION\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=True)\n",
    "scaler_model = scaler.fit(assembled_df)\n",
    "scaled_data = scaler_model.transform(assembled_df)\n",
    "\n",
    "# Cache the scaled data if used multiple times\n",
    "scaled_data.cache()\n",
    "\n",
    "scaled_data.select(\"scaledFeatures\").show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3cf7939-b025-4157-bb66-4c368b5faabb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. TEXT VECTORISATION WITH TF-IDF \n",
    "#generated with following ChatGPT prompt: Add tokenizer and count vectorisation with TF_IDF\n",
    "tokenizer = Tokenizer(inputCol=\"reviewText\", outputCol=\"words\")\n",
    "words_data = tokenizer.transform(df10)\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "filtered_data = remover.transform(words_data)\n",
    "\n",
    "vectorizer = CountVectorizer(inputCol=\"filtered\", outputCol=\"rawFeatures\")\n",
    "vector_model = vectorizer.fit(filtered_data)\n",
    "featurized_data = vector_model.transform(filtered_data)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features_text\")\n",
    "idf_model = idf.fit(featurized_data)\n",
    "tfidf_data = idf_model.transform(featurized_data)\n",
    "tfidf_data.select(\"features_text\").show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c40afd49-a0ff-427f-8ef0-a1379cf82dbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Machine Learning Model Implementation (Using SparkML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61255322-7dd8-4112-b80a-3a60c4684c6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import \n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a99600be-0d04-46df-aeff-c8d1b38d66f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# STEP 1: Label Creation\n",
    "\n",
    "# Label positive reviews as 1 (rating >= 4), negative as 0 (rating <= 2)\n",
    "# Neutral (rating == 3) is ignored\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Read the saved df10 from DBFS\n",
    "df10 = spark.read.parquet(\"/dbfs/df10_output\")\n",
    "\n",
    "df_labeled = df10.withColumn(\n",
    "    \"label\",\n",
    "    when(col(\"rating\") >= 4, 1).when(col(\"rating\") <= 2, 0)\n",
    ").filter(col(\"label\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7788cac5-86fc-4d42-9d93-8dfe7fd8adb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# STEP 2: Text Feature Extraction (TF-IDF)\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF\n",
    "\n",
    "# Tokenize review text\n",
    "tokenizer = Tokenizer(inputCol=\"reviewText\", outputCol=\"words\")\n",
    "tokenized = tokenizer.transform(df_labeled)\n",
    "\n",
    "# Remove stopwords\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "filtered = remover.transform(tokenized)\n",
    "\n",
    "# Convert words to term frequency vectors\n",
    "vectorizer = CountVectorizer(inputCol=\"filtered\", outputCol=\"rawFeatures\", vocabSize=5000)\n",
    "vector_model = vectorizer.fit(filtered)\n",
    "featurized = vector_model.transform(filtered)\n",
    "\n",
    "# Apply TF-IDF to scale frequencies\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idf_model = idf.fit(featurized)\n",
    "tfidf_data = idf_model.transform(featurized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ade9cae-8fd0-41fe-ac6b-aa5a60eca817",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# STEP 3: Train/Test Split\n",
    "\n",
    "# Use only necessary columns\n",
    "final_data = tfidf_data.select(\"features\", \"label\")\n",
    "\n",
    "# Split into train and test datasets\n",
    "train_data, test_data = final_data.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83da044d-8558-4eb6-b493-aa81f98ceed1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# STEP 4: Model Training\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create a logistic regression model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\n",
    "\n",
    "# Train the model\n",
    "lr_model = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40050fc9-c1ba-4170-97d3-95c0b4ade765",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# STEP 5: Predictions and Evaluation\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Make predictions\n",
    "predictions = lr_model.transform(test_data)\n",
    "\n",
    "# Evaluate using Accuracy and F1 Score\n",
    "evaluator_acc = MulticlassClassificationEvaluator(metricName=\"accuracy\", labelCol=\"label\", predictionCol=\"prediction\")\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(metricName=\"f1\", labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "accuracy = evaluator_acc.evaluate(predictions)\n",
    "f1 = evaluator_f1.evaluate(predictions)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "511ff377-aa0b-412b-8751-0e95ee834d2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Optimization & Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "feea9f44-c70b-42cf-a994-ac57c119fc27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Import required libraries\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63a1b4ec-ce42-4c5a-adbf-9278075bbb8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Create a binary classification evaluator\n",
    "# - It will be used to measure the model’s performance\n",
    "# - We use 'areaUnderROC' to evaluate classification quality\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "349543a1-28f0-4558-a027-dfe927d8d0f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 3: Build a parameter grid\n",
    "# - This defines multiple combinations of hyperparameters\n",
    "# - We’ll try different values of:\n",
    "#   regParam: controls regularization strength (smaller = less penalty)\n",
    "#   elasticNetParam: 0 = L2 (Ridge), 1 = L1 (Lasso), 0.5 = mix of both\n",
    "\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 0.5]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba5024ed-d04a-4285-a173-13babcda28d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 4: Create a CrossValidator\n",
    "# - It performs k-fold cross-validation (here, 3-fold)\n",
    "# - It will test all combinations from the parameter grid\n",
    "# - It returns the best model based on evaluator metric\n",
    "\n",
    "crossval = CrossValidator(\n",
    "    estimator=lr,                      # Our base model (Logistic Regression)\n",
    "    estimatorParamMaps=param_grid,     # Our hyperparameter grid\n",
    "    evaluator=evaluator,               # Evaluation strategy\n",
    "    numFolds=3                         # Number of folds for cross-validation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "431c9374-51cc-40a7-9447-62afa8fb023d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 5: Train the model using cross-validation\n",
    "# - This might take a few minutes\n",
    "\n",
    "cv_model = crossval.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85aee618-e3b7-466c-ae39-1960cb4239d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 6: Use the best model to make predictions on test data\n",
    "\n",
    "predictions_cv = cv_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9b6ffb5-37c7-480f-bbaa-90e0d2eea901",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 7: Evaluate the final tuned model\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Accuracy\n",
    "evaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator_acc.evaluate(predictions_cv)\n",
    "\n",
    "# F1 Score\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1_score = evaluator_f1.evaluate(predictions_cv)\n",
    "\n",
    "# Print final results\n",
    "print(\"===== Tuned Model Performance =====\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddbebf7a-daf3-4df9-8252-e395b4e23c5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 8: Print the best hyperparameters\n",
    "\n",
    "best_model = cv_model.bestModel\n",
    "\n",
    "print(\"\\n===== Best Hyperparameters Found =====\")\n",
    "print(\"Best Regularization Parameter (regParam):\", best_model._java_obj.getRegParam())\n",
    "print(\"Best ElasticNet Parameter (elasticNetParam):\", best_model._java_obj.getElasticNetParam())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e17329d1-05ae-4ae8-8a2f-d4cd16b14346",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ML Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f636a377-5bb5-40ca-9636-bd757250a389",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cache the training dataset to avoid recomputation\n",
    "train_data.cache()\n",
    "\n",
    "# Repartition the data to improve parallelism (if needed)\n",
    "train_data = train_data.repartition(4)\n",
    "\n",
    "# Persist final TF-IDF dataset if reused often\n",
    "tfidf_data.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0400c4c-360e-42cb-bf2f-381f3d15c305",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# 2. Collect true & predicted labels\n",
    "pdf_cm = predictions_cv.select(\"label\", \"prediction\").toPandas()\n",
    "\n",
    "# 3. Compute confusion matrix\n",
    "cm = confusion_matrix(pdf_cm[\"label\"], pdf_cm[\"prediction\"], labels=[0,1])\n",
    "\n",
    "# 4. Plot heatmap\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=[\"Pred Neg\",\"Pred Pos\"],\n",
    "    yticklabels=[\"True Neg\",\"True Pos\"]\n",
    ")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix: Tuned Logistic Regression\")\n",
    "plt.tight_layout()\n",
    "display(plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b19a8e7-1c0a-4a5f-b1c8-4799ec4a1ba4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Import\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# 2. Extract labels & scores\n",
    "pdf_roc = predictions_cv.select(\"label\", \"probability\").toPandas()\n",
    "y_true = pdf_roc[\"label\"]\n",
    "y_score = pdf_roc[\"probability\"].apply(lambda v: float(v[1]))\n",
    "\n",
    "# 3. Compute ROC metrics\n",
    "fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# 4. Plot ROC\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.3f}\")\n",
    "plt.plot([0,1], [0,1], linestyle='--', color='gray')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve: Tuned Logistic Regression\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "display(plt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84d04f91-9612-48e6-a702-0e40b16c1dfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Import\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "# 2. Compute PR metrics\n",
    "precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
    "avg_prec = average_precision_score(y_true, y_score)\n",
    "\n",
    "# 3. Plot PR curve\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(recall, precision, label=f\"AP = {avg_prec:.3f}\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision–Recall Curve\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.tight_layout()\n",
    "display(plt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbab3bbd-51b1-4dbe-866c-b4653c114227",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Evaluate baseline (if not already)\n",
    "pred_base = lr_model.transform(test_data)\n",
    "base_f1 = evaluator_f1.evaluate(pred_base)\n",
    "\n",
    "# 2. Evaluate tuned\n",
    "pred_tuned = predictions_cv.cache()\n",
    "tuned_f1 = evaluator_f1.evaluate(pred_tuned)\n",
    "\n",
    "# 3. Build DataFrame\n",
    "import pandas as pd\n",
    "df_cmp = pd.DataFrame({\n",
    "    \"Model\": [\"Baseline LR\", \"Tuned LR\"],\n",
    "    \"F1 Score\": [base_f1, tuned_f1]\n",
    "})\n",
    "\n",
    "# 4. Plot comparison\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(df_cmp[\"Model\"], df_cmp[\"F1 Score\"])\n",
    "plt.ylim(0,1)\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.title(\"Baseline vs. Tuned Logistic Regression\")\n",
    "plt.tight_layout()\n",
    "display(plt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f0f6f4b-e552-4c89-a41e-0ec7b6bce41a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Extract vocabulary & coefficients\n",
    "vocab = vector_model.vocabulary            # list of terms\n",
    "coeffs = lr_model.coefficients.toArray()   # NumPy array\n",
    "\n",
    "# 2. Build Pandas DataFrame\n",
    "import pandas as pd\n",
    "df_feat = pd.DataFrame({\n",
    "    \"term\": vocab,\n",
    "    \"coef\": coeffs\n",
    "})\n",
    "df_top = df_feat.reindex(df_feat.coef.abs().sort_values(ascending=False).index).head(20)\n",
    "\n",
    "# 3. Plot\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.barh(df_top[\"term\"], df_top[\"coef\"])\n",
    "plt.axvline(0, color='black', linewidth=0.8)\n",
    "plt.xlabel(\"Coefficient\")\n",
    "plt.title(\"Top 20 Influential Terms\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "display(plt)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Code",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}